{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import (XLMRobertaTokenizerFast, XLMRobertaForTokenClassification,\n",
    "                          DistilBertTokenizerFast, DistilBertForTokenClassification,\n",
    "                          BertTokenizerFast, BertForTokenClassification,\n",
    "                          Trainer, TrainingArguments)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Step 2: Load the dataset in CoNLL format\n",
    "def load_conll_dataset(file_path):\n",
    "    sentences, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        sentence, label = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, tag = line.split()\n",
    "                sentence.append(token)\n",
    "                label.append(tag)\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence, label = [], []\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    return sentences, labels\n",
    "\n",
    "# Load your CoNLL dataset\n",
    "file_path = r'/content/drive/MyDrive/labele_data.conll'  \n",
    "sentences, labels = load_conll_dataset(file_path)\n",
    "\n",
    "\n",
    "data = {'tokens': sentences, 'ner_tags': labels}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "unique_labels = set(tag for label in labels for tag in label)\n",
    "class_labels = ClassLabel(names=list(unique_labels))\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = [-100 if id is None else class_labels.str2int(label[id]) for id in word_ids]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def train_and_evaluate_model(model_name, model_class, tokenizer_class):\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    model = model_class.from_pretrained(model_name, num_labels=len(class_labels))\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n",
    "\n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/{model_name}',\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        eval_dataset=tokenized_dataset,  \n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    \n",
    "    predictions, label_ids, _ = trainer.predict(tokenized_dataset)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "    true_labels = [[class_labels.int2str(label) for label in label_row if label != -100] for label_row in label_ids]\n",
    "    pred_labels = [[class_labels.int2str(pred) for pred, label in zip(pred_row, label_row) if label != -100] for pred_row, label_row in zip(predictions, label_ids)]\n",
    "\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "\n",
    "models = [\n",
    "    ('xlm-roberta-base', XLMRobertaForTokenClassification, XLMRobertaTokenizerFast),\n",
    "    ('distilbert-base-multilingual-cased', DistilBertForTokenClassification, DistilBertTokenizerFast),\n",
    "    ('bert-base-multilingual-cased', BertForTokenClassification, BertTokenizerFast),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_class, tokenizer_class in models:\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    result = train_and_evaluate_model(model_name, model_class, tokenizer_class)\n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "best_model = results_df.loc[results_df['f1'].idxmax()]\n",
    "print(f\"Best performing model: {best_model['model']} with F1 Score: {best_model['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
